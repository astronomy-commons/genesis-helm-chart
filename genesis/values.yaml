rbac:
  enabled: true

aws-hub:
  jupyterhub:    
    hub:
      allowNamedServers: true
      image:
        tag: "0.9-445a953"
      extraConfig:
        run_user_as_root: |
          c.KubeSpawner.uid = 0
          c.Spawner.args.append("--allow-root")
        
        auth: |
          import oauthenticator
          from tornado import gen
          import os, secrets
          
          class MyGitHubAuth(oauthenticator.github.GitHubOAuthenticator):
            @gen.coroutine
            def authenticate(self, handler, data=None):
              userdict = yield super().authenticate(handler, data)
              return userdict

            @gen.coroutine
            def pre_spawn_start(self, user, spawner):
                """Pass upstream_token to spawner via environment variable"""
                auth_state = yield user.get_auth_state()
                if not auth_state:
                    # auth_state not enabled
                    return
                spawner.environment["NB_UID"] = str(auth_state["github_user"]["id"])
                spawner.environment["NB_USER"] = str(auth_state["github_user"]["login"])

          c.JupyterHub.authenticator_class = MyGitHubAuth
          c.Authenticator.enable_auth_state = True
          
          os.environ["JUPYTERHUB_CRYPT_KEY"] = secrets.token_hex(32)

    prePuller:
      hook:
        image:
          tag: "0.9-b51ffeb"

    singleuser:
      networkTools:
        image:
          tag: "0.9-b51ffeb"
      # set the hub to deploy an image with AXS installed
      image:
        name: 808034228930.dkr.ecr.us-west-2.amazonaws.com/jupyter-axs
        tag: demo-1567804976
      # extraEnv:
      #   KUBERNETES_NAMESPACE: "default"
      #   PUBLIC_URL: "hub.dirac.astro.washington.edu"
      #   CHOWN_HOME: "yes"
      #   CHOWN_HOME_OPTS: "-R"
      #   CHOWN_EXTRA: "/nfs"
      #   #,/usr/local/spark-labextension,/opt/conda/share/jupyter/lab"
      #   CHOWN_EXTRA_OPTS: "-R"
      #   SPARK_EXTERNAL_CONF_DIR: "/etc/config/spark"

      # assigns the notebook pod the service account called jupyter-spark-serviceaccount in the k8s cluster
      # this service account is created if rbac.enabled is set to true in this Helm chart
      # credentials are mounted in the notebook pod at /var/run/secrets/kubernetes.io/serviceaccount
      # allows Spark (and the user) to access the Kubernetes cluster via the API at https://kubernetes.default.svc:443
      serviceAccountName: jupyter-spark-serviceaccount
      storage:
        # mount the files spark-defaults.conf and spark-env.sh into the user notebook
        # these alter the start-up behavior of Spark
        extraVolumes:
        - name: "spark-config-volume"
          configMap:
            name: "spark-config"
        # consume the nfs PVC that comes with axs-hub
        - name: "nfs-volume"
          persistentVolumeClaim:
            claimName: "nfs"
        # jupyter configurations
        - name: "start-notebook-volume"
          configMap:
            name: "start-notebook.d"
        - name: "begin-notebook-volume"
          configMap:
            name: "begin-notebook.d"
        extraVolumeMounts:
        # mount spark configurations from configMap
        - name: "spark-config-volume"
          mountPath: "/usr/local/axs/conf/spark-defaults.conf.static"
          # subPath access the specified file within the config map
          subPath: "spark-defaults.conf"
        - name: "spark-config-volume"
          mountPath: "/usr/local/axs/conf/spark-env.sh.static"
          subPath: "spark-env.sh"
        # mount the efs-backed filesystem to /nfs
        # creates a folder for the user in /nfs and /nfs contains all other users
        - name: "nfs-volume"
          mountPath: "/nfs/{username}"
          # supPath creates a folder within the volume for the user
          subPath: "{username}"
        - name: "nfs-volume"
          mountPath: "/nfs"
        # jupyter configurations
        - name: "start-notebook-volume"
          mountPath: "/usr/local/bin/start-notebook.d"
        - name: "begin-notebook-volume"
          mountPath: "/usr/local/bin/begin-notebook.d"


spark-defaults.conf:
  000-s3-defaults: |
    # s3 access
    spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
  000-kubernetes-defaults: |
    # kubernetes options
    spark.submit.deployMode=client
    spark.master=k8s://https://kubernetes.default.svc:443
  000-scheduler-defaults: |
    # scheduling options (batch size, time out)
    spark.kubernetes.allocation.batch.size=100
    spark.scheduler.maxRegisteredResourcesWaitingTime=600s
    spark.scheduler.minRegisteredResourcesRatio=1.0
  000-executor-defaults: |
    # options for the executor pods
    spark.executor.memory=3500m
    spark.kubernetes.executor.request.cores=0.945
    spark.kubernetes.executor.limit.cores=1.0
  000-sql-defaults: |
    spark.sql.execution.arrow.enabled=true
  000-java-defaults: |
    spark.driver.extraJavaOptions -Dderby.system.home=/tmp/derby

spark-env.sh:

start-notebook:
  001-env-vars.sh: |
    export SPARK_PUBLIC_DNS="${PUBLIC_URL}${JUPYTERHUB_SERVICE_PREFIX}proxy/4040/jobs/"
    export JAVA_HOME=$(dirname $(dirname $(readlink -f $(which java))))
  
  002-spark-defaults.sh: |
    conf_file=$SPARK_HOME/conf/spark-defaults.conf.dynamic
    echo "spark.driver.host=$(hostname -i)" >> $conf_file
    echo "spark.ui.proxyBase=${JUPYTERHUB_SERVICE_PREFIX}proxy/4040" >> $conf_file
    echo "spark.kubernetes.executor.container.image=${JUPYTER_IMAGE}" >> $conf_file
    prefix=spark.executorEnv
    if [ -n "${NB_USER}" ]; then
      echo "${prefix}.NB_USER ${NB_USER}" >> $conf_file
      echo "spark.kubernetes.executor.podNamePrefix ${NB_USER}-spark" >> $conf_file
      echo "spark.kubernetes.driver.pod.name jupyter-${NB_USER}" | awk '{print tolower($0)}' >> $conf_file
    fi
    if [ -n "${NB_UID}" ]; then
      echo "${prefix}.NB_UID ${NB_UID}" >> $conf_file
    fi
    echo "${prefix}.JAVA_HOME=${JAVA_HOME}" >> $conf_file
  
  999-merge-spark-files.sh: |
    static_conf_file=$SPARK_HOME/conf/spark-defaults.conf.static
    dynamic_conf_file=$SPARK_HOME/conf/spark-defaults.conf.dynamic
    conf_file=$SPARK_HOME/conf/spark-defaults.conf

    if [ -f "$conf_file" ]; then
      rm -f $conf_file
    fi
    if [ -f "$static_conf_file" ]; then
      cat $static_conf_file >> $conf_file
    fi
    if [ -f "$dynamic_conf_file" ]; then
      echo "" >> $conf_file
      cat $dynamic_conf_file >> $conf_file
    fi

    static_env_file=$SPARK_HOME/conf/spark-env.sh.static
    dynamic_env_file=$SPARK_HOME/conf/spark-env.sh.dynamic
    env_file=$SPARK_HOME/conf/spark-env.sh

    if [ -f "$env_file" ]; then
      rm -f $env_file
    fi
    if [ -f "$static_env_file" ]; then
      cat $static_env_file >> $env_file
    fi
    if [ -f "$dynamic_env_file" ]; then
      echo "" >> $env_file
      cat $dynamic_env_file >> $env_file
    fi

